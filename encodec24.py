from datasets import load_dataset, Audio
from transformers import EncodecModel, AutoProcessor
import torchaudio

# dummy dataset, however you can swap this with an dataset on the ğŸ¤— hub or bring your own
librispeech_dummy = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation")


# load the model + processor (for pre-processing the audio)
model = EncodecModel.from_pretrained("facebook/encodec_24khz")
processor = AutoProcessor.from_pretrained("facebook/encodec_24khz")


# cast the audio data to the correct sampling rate for the model
# 'audio' ì—´ì„ Audio()ë¥¼ í™œìš©í•´ ì›í•˜ëŠ” srì— ë§ì¶° ë°ì´í„° íƒ€ì…ìœ¼ë¡œ ë³€í™˜
librispeech_dummy = librispeech_dummy.cast_column("audio", Audio(sampling_rate=processor.sampling_rate))

#len(librispeech_dummy)

for i in range(5) : 
    audio_sample = librispeech_dummy[i]["audio"]["array"]
    print(audio_sample.shape)

    # pre-process the inputs
    inputs = processor(raw_audio=audio_sample, sampling_rate=processor.sampling_rate, return_tensors="pt")

    # or the equivalent with a forward pass
    audio_values = model(inputs["input_values"], inputs["padding_mask"]).audio_values

    audio_codes = model(inputs["input_values"], inputs["padding_mask"]).audio_codes


    ### ì˜¤ë””ì˜¤ë¡œ ì¶”ì¶œ
    audio_values = audio_values.squeeze()

    #ì˜¤ë””ì˜¤íŒŒì¼ë¡œ ì €ì¥
    torchaudio.save(f"spoof_test/encodec24_spoof_{i:03}.wav", audio_values.unsqueeze(0), processor.sampling_rate)

