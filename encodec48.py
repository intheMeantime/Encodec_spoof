from datasets import load_dataset, Audio
from transformers import EncodecModel, AutoProcessor
import torchaudio
import torch

# dummy dataset, however you can swap this with an dataset on the ğŸ¤— hub or bring your own
librispeech_dummy = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation")


# load the model + processor (for pre-processing the audio)
model = EncodecModel.from_pretrained("facebook/encodec_48khz")
processor = AutoProcessor.from_pretrained("facebook/encodec_48khz")


# cast the audio data to the correct sampling rate for the model
# 'audio' ì—´ì„ Audio()ë¥¼ í™œìš©í•´ ì›í•˜ëŠ” srì— ë§ì¶° ë°ì´í„° íƒ€ì…ìœ¼ë¡œ ë³€í™˜
librispeech_dummy = librispeech_dummy.cast_column("audio", Audio(sampling_rate=processor.sampling_rate))

#len(librispeech_dummy)

for i in range(5) : 
    audio_sample = librispeech_dummy[i]["audio"]["array"]

    #print(audio_sample.shape)

    audio_sample = torch.tensor(audio_sample)  # NumPy ë°°ì—´ì„ PyTorch í…ì„œë¡œ ë³€í™˜
     # ì˜¤ë””ì˜¤ ìƒ˜í”Œì´ 1ì°¨ì›ì¼ ê²½ìš° ìŠ¤í…Œë ˆì˜¤ë¡œ ë³€í™˜
    if audio_sample.ndim == 1:  # ëª¨ë…¸ ì˜¤ë””ì˜¤
        audio_sample = audio_sample.unsqueeze(0).repeat(2, 1)  # ìŠ¤í…Œë ˆì˜¤ë¡œ ë³€í™˜
    elif audio_sample.ndim > 1:  # ì—¬ëŸ¬ ì±„ë„ì¼ ê²½ìš°
        audio_sample = audio_sample.mean(dim=0, keepdim=True)  # ëª¨ë…¸ë¡œ ë³€í™˜
        audio_sample = audio_sample.repeat(2, 1)  # ìŠ¤í…Œë ˆì˜¤ë¡œ ë³€í™˜

    # pre-process the inputs
    inputs = processor(raw_audio=audio_sample, sampling_rate=processor.sampling_rate, return_tensors="pt")
    
    # or the equivalent with a forward pass
    audio_values = model(inputs["input_values"], inputs["padding_mask"]).audio_values

    # audio_codes = model(inputs["input_values"], inputs["padding_mask"]).audio_codes
    print(audio_values.shape)

    #ì˜¤ë””ì˜¤íŒŒì¼ë¡œ ì €ì¥
    torchaudio.save(f"spoof_test/encodec48_spoof_{i:03}.wav", audio_values.squeeze(0), processor.sampling_rate)

